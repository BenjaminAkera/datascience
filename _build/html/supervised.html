

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>6. Supervised Learning &mdash; Data Science 0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Data Science 0.1 documentation" href="index.html"/>
        <link rel="next" title="7. Unsupervised Learning" href="unsupervised.html"/>
        <link rel="prev" title="5. Tests of Association" href="association.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Data Science
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="general.html">1. General Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="assumptions.html">2. Tests for Assumptions</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">3. In-Built Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="difference.html">4. Tests of Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="association.html">5. Tests of Association</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">6. Supervised Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#classification">6.1. Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-nearest-neighbours-knn">6.1.1. K Nearest Neighbours (KNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#decision-tree">6.1.2. Decision Tree</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-forest">6.1.3. Random Forest</a></li>
<li class="toctree-l3"><a class="reference internal" href="#logistic-regression">6.1.4. Logistic Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#support-vector-machine">6.1.5. Support Vector Machine</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#regression">6.2. Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ols-regression">6.2.1. OLS Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ridge-regression">6.2.2. Ridge Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lasso-regression">6.2.3. LASSO Regression</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="unsupervised.html">7. Unsupervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="decomposition.html">8. Time Series Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting.html">9. Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="resources.html">10. Resources</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Data Science</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>6. Supervised Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/supervised.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="supervised-learning">
<h1>6. Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="classification">
<h2>6.1. Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="k-nearest-neighbours-knn">
<h3>6.1.1. K Nearest Neighbours (KNN)<a class="headerlink" href="#k-nearest-neighbours-knn" title="Permalink to this headline">¶</a></h3>
<div class="topic">
<p class="topic-title first"><strong>Parameters Selection</strong></p>
<ol class="arabic simple">
<li><code class="docutils literal"><span class="pre">Distance</span> <span class="pre">Metric:</span></code> Eclidean Distance (default). In sklearn it is known as (Minkowski with p = 2)</li>
<li><code class="docutils literal"><span class="pre">How</span> <span class="pre">many</span> <span class="pre">nearest</span> <span class="pre">neighbour</span> <span class="pre">to</span> <span class="pre">look</span> <span class="pre">at:</span></code> k=1 very specific, k=5 more general model. Use nearest k data points to determine classification</li>
<li><code class="docutils literal"><span class="pre">Weighting</span> <span class="pre">function</span> <span class="pre">on</span> <span class="pre">neighbours:</span></code> (optional)</li>
<li><code class="docutils literal"><span class="pre">How</span> <span class="pre">to</span> <span class="pre">aggregate</span> <span class="pre">class</span> <span class="pre">of</span> <span class="pre">neighbour</span> <span class="pre">points:</span></code> Simple majority (default)</li>
</ol>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">#### IMPORT MODULES ####</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">KNeighborsClassifier</span>



<span class="c1">#### TRAIN TEST SPLIT ####</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>



<span class="c1">#### CREATE MODEL ####</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>



<span class="c1">#### FIT MODEL ####</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1">#KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,</span>
<span class="c1">#     metric_params=None, n_jobs=1, n_neighbors=5, p=2,</span>
<span class="c1">#     weights=&#39;uniform&#39;)</span>



<span class="c1">#### TEST MODEL ####</span>
<span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">0.53333333333333333</span>
</pre></div>
</div>
</div>
<div class="section" id="decision-tree">
<h3>6.1.2. Decision Tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">¶</a></h3>
<p>Uses gini index to split the data at binary level.</p>
<p><strong>Strengths:</strong> Can select a large number of features that best determine the targets.
<strong>Weakness:</strong> Tends to overfit the data as it will split till the end.
Pruning can be done to remove the leaves to prevent overfitting but that is not available in sklearn.
Small changes in data can lead to different splits. Not very reproducible for future data (see random forest).</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">###### IMPORT MODULES #### ###</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>



<span class="c1">#### TRAIN TEST SPLIT ####</span>
<span class="n">train_predictor</span><span class="p">,</span> <span class="n">test_predictor</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> \
<span class="n">train_test_split</span><span class="p">(</span><span class="n">predictor</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">test_predictor</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">train_predictor</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">38</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="p">(</span><span class="mi">112</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>



<span class="c1">#### CREATE MODEL ####</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>



<span class="c1">#### FIT MODEL ####</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_predictor</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">model</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">presort</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">splitter</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>



<span class="c1">#### TEST MODEL ####</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_predictor</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span><span class="n">predictions</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;%&#39;</span>
<span class="p">[[</span><span class="mi">14</span>  <span class="mi">0</span>  <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">0</span> <span class="mi">13</span>  <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">0</span>  <span class="mi">1</span> <span class="mi">10</span><span class="p">]]</span>
<span class="mf">97.3684210526</span> <span class="o">%</span>


<span class="c1">#### SCORE MODEL ####</span>
<span class="c1"># it is easier to use this package that does everything nicely for a perfect confusion matrix</span>
<span class="kn">from</span> <span class="nn">pandas_confusion</span> <span class="k">import</span> <span class="n">ConfusionMatrix</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">ConfusionMatrix</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="n">Predicted</span>   <span class="n">setosa</span>  <span class="n">versicolor</span>  <span class="n">virginica</span>  <span class="n">__all__</span>
<span class="n">Actual</span>
<span class="n">setosa</span>          <span class="mi">14</span>           <span class="mi">0</span>          <span class="mi">0</span>       <span class="mi">14</span>
<span class="n">versicolor</span>       <span class="mi">0</span>          <span class="mi">13</span>          <span class="mi">0</span>       <span class="mi">13</span>
<span class="n">virginica</span>        <span class="mi">0</span>           <span class="mi">1</span>         <span class="mi">10</span>       <span class="mi">11</span>
<span class="n">__all__</span>         <span class="mi">14</span>          <span class="mi">14</span>         <span class="mi">10</span>       <span class="mi">38</span>



<span class="c1">####### FEATURE IMPORTANCE #### ####</span>
<span class="n">df2</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">df2</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">petal</span> <span class="n">width</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>      <span class="mf">0.952542</span>
<span class="n">petal</span> <span class="n">length</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>     <span class="mf">0.029591</span>
<span class="n">sepal</span> <span class="n">length</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>     <span class="mf">0.017867</span>
<span class="n">sepal</span> <span class="n">width</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>      <span class="mf">0.000000</span>
</pre></div>
</div>
</div>
<div class="section" id="random-forest">
<h3>6.1.3. Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h3>
<p>An ensemble of decision trees.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">###### IMPORT MODULES #### ###</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span>



<span class="c1">#### TRAIN TEST SPLIT ####</span>
<span class="n">train_feature</span><span class="p">,</span> <span class="n">test_feature</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> \
<span class="n">train_test_split</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">train_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">test_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">404</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>
<span class="p">(</span><span class="mi">102</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>


<span class="c1">#### CREATE MODEL ####</span>
<span class="c1"># use 100 decision trees</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>



<span class="c1">#### FIT MODEL ####</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_feature</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">model</span>
<span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span>
            <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>



<span class="c1">#### TEST MODEL ####</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_feature</span><span class="p">)</span>



<span class="c1">#### SCORE MODEL ####</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;%&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s1">&#39;confusion matrix&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">accuracy</span>
<span class="mf">82.3529411765</span> <span class="o">%</span>
<span class="n">confusion</span> <span class="n">matrix</span>
<span class="p">[[</span><span class="mi">21</span>  <span class="mi">0</span>  <span class="mi">3</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">0</span> <span class="mi">21</span>  <span class="mi">4</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">8</span>  <span class="mi">3</span> <span class="mi">42</span><span class="p">]]</span>



<span class="c1">####### FEATURE IMPORTANCE #### ####</span>
<span class="c1"># rank the importance of features</span>
<span class="n">df2</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">df2</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">RM</span>    <span class="mf">0.225612</span>
<span class="n">LSTAT</span> <span class="mf">0.192478</span>
<span class="n">CRIM</span>  <span class="mf">0.108510</span>
<span class="n">DIS</span>   <span class="mf">0.088056</span>
<span class="n">AGE</span>   <span class="mf">0.074202</span>
<span class="n">NOX</span>   <span class="mf">0.067718</span>
<span class="n">B</span>     <span class="mf">0.057706</span>
<span class="n">PTRATIO</span>       <span class="mf">0.051702</span>
<span class="n">TAX</span>   <span class="mf">0.047568</span>
<span class="n">INDUS</span> <span class="mf">0.037871</span>
<span class="n">RAD</span>   <span class="mf">0.026538</span>
<span class="n">ZN</span>    <span class="mf">0.012635</span>
<span class="n">CHAS</span>  <span class="mf">0.009405</span>



<span class="c1">#### GRAPHS ####</span>

<span class="c1"># see how many decision trees are minimally required make the accuarcy consistent</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">trees</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">accuracy</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trees</span><span class="p">)):</span>
  <span class="n">clf</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">model</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_feature</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
  <span class="n">predictions</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_feature</span><span class="p">)</span>
  <span class="n">accuracy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trees</span><span class="p">,</span><span class="n">accuracy</span><span class="p">)</span>

<span class="c1"># well, seems like more than 10 trees will have a consistent accuracy of 0.82.</span>
<span class="c1"># Guess there&#39;s no need to have an ensemble of 100 trees!</span>
</pre></div>
</div>
<img alt="_images/randomforest.png" src="_images/randomforest.png" />
</div>
<div class="section" id="logistic-regression">
<h3>6.1.4. Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h3>
<p>Binary output.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">#### IMPORT MODULES ####</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>



<span class="c1">#### FIT MODEL ####</span>
<span class="n">lreg</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">Logit</span><span class="p">(</span><span class="n">df3</span><span class="p">[</span><span class="s1">&#39;diameter_cut&#39;</span><span class="p">],</span> <span class="n">df3</span><span class="p">[</span><span class="n">trainC</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span> <span class="n">lreg</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>


<span class="n">Optimization</span> <span class="n">terminated</span> <span class="n">successfully</span><span class="o">.</span>
       <span class="n">Current</span> <span class="n">function</span> <span class="n">value</span><span class="p">:</span> <span class="mf">0.518121</span>
       <span class="n">Iterations</span> <span class="mi">6</span>
                           <span class="n">Logit</span> <span class="n">Regression</span> <span class="n">Results</span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span> <span class="n">Variable</span><span class="p">:</span>           <span class="n">diameter_cut</span>   <span class="n">No</span><span class="o">.</span> <span class="n">Observations</span><span class="p">:</span>                <span class="mi">18067</span>
<span class="n">Model</span><span class="p">:</span>                          <span class="n">Logit</span>   <span class="n">Df</span> <span class="n">Residuals</span><span class="p">:</span>                    <span class="mi">18065</span>
<span class="n">Method</span><span class="p">:</span>                           <span class="n">MLE</span>   <span class="n">Df</span> <span class="n">Model</span><span class="p">:</span>                            <span class="mi">1</span>
<span class="n">Date</span><span class="p">:</span>                <span class="n">Thu</span><span class="p">,</span> <span class="mi">04</span> <span class="n">Aug</span> <span class="mi">2016</span>   <span class="n">Pseudo</span> <span class="n">R</span><span class="o">-</span><span class="n">squ</span><span class="o">.</span><span class="p">:</span>                  <span class="mf">0.2525</span>
<span class="n">Time</span><span class="p">:</span>                        <span class="mi">14</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">14</span>   <span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span>                <span class="o">-</span><span class="mf">9360.9</span>
<span class="n">converged</span><span class="p">:</span>                       <span class="kc">True</span>   <span class="n">LL</span><span class="o">-</span><span class="n">Null</span><span class="p">:</span>                       <span class="o">-</span><span class="mf">12523.</span>
                                        <span class="n">LLR</span> <span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="p">:</span>                     <span class="mf">0.000</span>
<span class="o">================================================================================</span>
                   <span class="n">coef</span>    <span class="n">std</span> <span class="n">err</span>          <span class="n">z</span>      <span class="n">P</span><span class="o">&gt;|</span><span class="n">z</span><span class="o">|</span>      <span class="p">[</span><span class="mf">95.0</span><span class="o">%</span> <span class="n">Conf</span><span class="o">.</span> <span class="n">Int</span><span class="o">.</span><span class="p">]</span>
<span class="o">--------------------------------------------------------------------------------</span>
<span class="n">depth</span>            <span class="mf">4.2529</span>      <span class="mf">0.067</span>     <span class="mf">63.250</span>      <span class="mf">0.000</span>         <span class="mf">4.121</span>     <span class="mf">4.385</span>
<span class="n">layers_YESNO</span>    <span class="o">-</span><span class="mf">2.1102</span>      <span class="mf">0.037</span>    <span class="o">-</span><span class="mf">57.679</span>      <span class="mf">0.000</span>        <span class="o">-</span><span class="mf">2.182</span>    <span class="o">-</span><span class="mf">2.039</span>
<span class="o">================================================================================</span>



<span class="c1">#### CONFIDENCE INTERVALS ####</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">lreg</span><span class="o">.</span><span class="n">params</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">lreg</span><span class="o">.</span><span class="n">conf_int</span><span class="p">()</span>
<span class="n">conf</span><span class="p">[</span><span class="s1">&#39;OR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span>
<span class="n">conf</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Lower CI&#39;</span><span class="p">,</span> <span class="s1">&#39;Upper CI&#39;</span><span class="p">,</span> <span class="s1">&#39;OR&#39;</span><span class="p">]</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">conf</span><span class="p">))</span>

<span class="n">Lower</span> <span class="n">CI</span>   <span class="n">Upper</span> <span class="n">CI</span>         <span class="n">OR</span>
<span class="n">depth</span>         <span class="mf">61.625434</span>  <span class="mf">80.209893</span>  <span class="mf">70.306255</span>
<span class="n">layers_YESNO</span>   <span class="mf">0.112824</span>   <span class="mf">0.130223</span>   <span class="mf">0.121212</span>
</pre></div>
</div>
</div>
<div class="section" id="support-vector-machine">
<h3>6.1.5. Support Vector Machine<a class="headerlink" href="#support-vector-machine" title="Permalink to this headline">¶</a></h3>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
</div>
<div class="section" id="regression">
<h2>6.2. Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h2>
<div class="section" id="ols-regression">
<h3>6.2.1. OLS Regression<a class="headerlink" href="#ols-regression" title="Permalink to this headline">¶</a></h3>
<p>Ordinary Least Squares Regression or OLS Regression is the most basic form and fundamental of regression.
Best fit line <code class="docutils literal"><span class="pre">ŷ</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">bx</span></code> is drawn based on the ordinary least squares method. i.e., least total area of squares (sum of squares) with length from each x,y point to regresson line.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;diameter ~ depth&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span> <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>



<span class="n">OLS</span> <span class="n">Regression</span> <span class="n">Results</span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span> <span class="n">Variable</span><span class="p">:</span>               <span class="n">diameter</span>   <span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span>                       <span class="mf">0.512</span>
<span class="n">Model</span><span class="p">:</span>                            <span class="n">OLS</span>   <span class="n">Adj</span><span class="o">.</span> <span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span>                  <span class="mf">0.512</span>
<span class="n">Method</span><span class="p">:</span>                 <span class="n">Least</span> <span class="n">Squares</span>   <span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span>                 <span class="mf">1.895e+04</span>
<span class="n">Date</span><span class="p">:</span>                <span class="n">Tue</span><span class="p">,</span> <span class="mi">02</span> <span class="n">Aug</span> <span class="mi">2016</span>   <span class="n">Prob</span> <span class="p">(</span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">):</span>               <span class="mf">0.00</span>
<span class="n">Time</span><span class="p">:</span>                        <span class="mi">17</span><span class="p">:</span><span class="mi">10</span><span class="p">:</span><span class="mi">34</span>   <span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span>                <span class="o">-</span><span class="mf">51812.</span>
<span class="n">No</span><span class="o">.</span> <span class="n">Observations</span><span class="p">:</span>               <span class="mi">18067</span>   <span class="n">AIC</span><span class="p">:</span>                         <span class="mf">1.036e+05</span>
<span class="n">Df</span> <span class="n">Residuals</span><span class="p">:</span>                   <span class="mi">18065</span>   <span class="n">BIC</span><span class="p">:</span>                         <span class="mf">1.036e+05</span>
<span class="n">Df</span> <span class="n">Model</span><span class="p">:</span>                           <span class="mi">1</span>
<span class="n">Covariance</span> <span class="n">Type</span><span class="p">:</span>            <span class="n">nonrobust</span>
<span class="o">==============================================================================</span>
<span class="n">coef</span>    <span class="n">std</span> <span class="n">err</span>          <span class="n">t</span>      <span class="n">P</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span>      <span class="p">[</span><span class="mf">95.0</span><span class="o">%</span> <span class="n">Conf</span><span class="o">.</span> <span class="n">Int</span><span class="o">.</span><span class="p">]</span>
<span class="o">------------------------------------------------------------------------------</span>
<span class="n">Intercept</span>      <span class="mf">2.2523</span>      <span class="mf">0.054</span>     <span class="mf">41.656</span>      <span class="mf">0.000</span>         <span class="mf">2.146</span>     <span class="mf">2.358</span>
<span class="n">depth</span>         <span class="mf">11.5836</span>      <span class="mf">0.084</span>    <span class="mf">137.675</span>      <span class="mf">0.000</span>        <span class="mf">11.419</span>    <span class="mf">11.749</span>
<span class="o">==============================================================================</span>
<span class="n">Omnibus</span><span class="p">:</span>                    <span class="mf">12117.030</span>   <span class="n">Durbin</span><span class="o">-</span><span class="n">Watson</span><span class="p">:</span>                   <span class="mf">0.673</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">Omnibus</span><span class="p">):</span>                  <span class="mf">0.000</span>   <span class="n">Jarque</span><span class="o">-</span><span class="n">Bera</span> <span class="p">(</span><span class="n">JB</span><span class="p">):</span>           <span class="mf">391356.565</span>
<span class="n">Skew</span><span class="p">:</span>                           <span class="mf">2.771</span>   <span class="n">Prob</span><span class="p">(</span><span class="n">JB</span><span class="p">):</span>                         <span class="mf">0.00</span>
<span class="n">Kurtosis</span><span class="p">:</span>                      <span class="mf">25.117</span>   <span class="n">Cond</span><span class="o">.</span> <span class="n">No</span><span class="o">.</span>                         <span class="mf">3.46</span>
<span class="o">==============================================================================</span>

<span class="n">Warnings</span><span class="p">:</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="n">Standard</span> <span class="n">Errors</span> <span class="n">assume</span> <span class="n">that</span> <span class="n">the</span> <span class="n">covariance</span> <span class="n">matrix</span> <span class="n">of</span> <span class="n">the</span> <span class="n">errors</span> <span class="ow">is</span> <span class="n">correctly</span> <span class="n">specified</span><span class="o">.</span>
</pre></div>
</div>
</div>
<div class="section" id="ridge-regression">
<h3>6.2.2. Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="lasso-regression">
<h3>6.2.3. LASSO Regression<a class="headerlink" href="#lasso-regression" title="Permalink to this headline">¶</a></h3>
<p>Least absolute shrinkage and selection operator regression, or LASSO regression, has a unique penalty parameter, lambda that <em>change unimportant features (their regression coefficients) into 0</em>.
This helps to prevent <em>overfitting</em>.</p>
<ul class="simple">
<li>Prevent overfitting.</li>
<li>Uses regularisation.</li>
<li>Uses a penalty parameter lambda to change unimportant features (their regression coefficients) into 0. When lambda = 0, then it is a normal OLS regression. (Note sklearn name it as alpha instead)<ol class="loweralpha">
<li>Bias increase &amp; variability decreases when lambda increases.</li>
<li>Useful when there are many features (explanatory variables).</li>
<li>Have to standardize all features so that they have mean 0 and std error 1.</li>
<li>Have several algorithms: LAR (Least Angle Regression). Starts w 0 predictors &amp; add each predictor that is most correlated at each step.</li>
</ol>
</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">sklearn define lambda as alpha instead.</p>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">#### IMPORT MODULES ####</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">py</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LassoLarsCV</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_boston</span>



<span class="c1">#### NORMALIZATION ####</span>
<span class="c1"># standardise the means to 0 and standard error to 1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span> <span class="c1"># df.columns[:-1] = dataframe for all features</span>
  <span class="n">df</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float64&#39;</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>



<span class="c1">#### TRAIN TEST SPLIT ####</span>
<span class="n">train_feature</span><span class="p">,</span> <span class="n">test_feature</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> \
<span class="n">train_test_split</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">train_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">test_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="mi">404</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="mi">102</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>



<span class="c1">#### CREATE MODEL ####</span>
<span class="c1"># Fit the LASSO LAR regression model</span>
<span class="c1"># cv=10; use k-fold cross validation</span>
<span class="c1"># precompute; True=model will be faster if dataset is large</span>
<span class="n">model</span><span class="o">=</span><span class="n">LassoLarsCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">precompute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>



<span class="c1">#### FIT MODEL ####</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_feature</span><span class="p">,</span><span class="n">train_target</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">model</span>
<span class="n">LassoLarsCV</span><span class="p">(</span><span class="n">copy_X</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">2.2204460492503131e-16</span><span class="p">,</span>
      <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">max_n_alphas</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
      <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">positive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">precompute</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>



<span class="c1">#### ANALYSE COEFFICIENTS ####</span>
<span class="n">Compare</span> <span class="n">the</span> <span class="n">regression</span> <span class="n">coefficients</span><span class="p">,</span> <span class="ow">and</span> <span class="n">see</span> <span class="n">which</span> <span class="n">one</span> <span class="n">LASSO</span> <span class="n">removed</span><span class="o">.</span>
<span class="n">LSTAT</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">most</span> <span class="n">important</span> <span class="n">predictor</span><span class="p">,</span> <span class="n">followed</span> <span class="n">by</span> <span class="n">RM</span><span class="p">,</span> <span class="n">DIS</span><span class="p">,</span> <span class="ow">and</span> <span class="n">RAD</span><span class="o">.</span> <span class="n">AGE</span> <span class="ow">is</span> <span class="n">removed</span> <span class="n">by</span> <span class="n">LASSO</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">df2</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">feature</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">df2</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">RM</span>    <span class="mf">3.050843</span>
<span class="n">RAD</span>   <span class="mf">2.040252</span>
<span class="n">ZN</span>    <span class="mf">1.004318</span>
<span class="n">B</span>     <span class="mf">0.629933</span>
<span class="n">CHAS</span>  <span class="mf">0.317948</span>
<span class="n">INDUS</span> <span class="mf">0.225688</span>
<span class="n">AGE</span>   <span class="mf">0.000000</span>
<span class="n">CRIM</span>  <span class="o">-</span><span class="mf">0.770291</span>
<span class="n">NOX</span>   <span class="o">-</span><span class="mf">1.617137</span>
<span class="n">TAX</span>   <span class="o">-</span><span class="mf">1.731576</span>
<span class="n">PTRATIO</span>       <span class="o">-</span><span class="mf">1.923485</span>
<span class="n">DIS</span>   <span class="o">-</span><span class="mf">2.733660</span>
<span class="n">LSTAT</span> <span class="o">-</span><span class="mf">3.878356</span>



<span class="c1">#### SCORE MODEL ####</span>
<span class="c1"># MSE from training and test data</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">mean_squared_error</span>
<span class="n">train_error</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">train_target</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_feature</span><span class="p">))</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_feature</span><span class="p">))</span>

<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;training data MSE&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_error</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;test data MSE&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_error</span><span class="p">)</span>

<span class="c1"># MSE closer to 0 are better</span>
<span class="c1"># test dataset is less accurate as expected</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">training</span> <span class="n">data</span> <span class="n">MSE</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">20.7279948891</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">test</span> <span class="n">data</span> <span class="n">MSE</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">28.3767672242</span>


<span class="c1"># R-square from training and test data</span>
<span class="n">rsquared_train</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_feature</span><span class="p">,</span><span class="n">train_target</span><span class="p">)</span>
<span class="n">rsquared_test</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_feature</span><span class="p">,</span><span class="n">test_target</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;training data R-square&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rsquared_train</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;test data R-square&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rsquared_test</span><span class="p">)</span>

<span class="c1"># test data explained 65% of the predictors</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">training</span> <span class="n">data</span> <span class="n">R</span><span class="o">-</span><span class="n">square</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">0.755337444405</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">test</span> <span class="n">data</span> <span class="n">R</span><span class="o">-</span><span class="n">square</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">0.657019301268</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="unsupervised.html" class="btn btn-neutral float-right" title="7. Unsupervised Learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="association.html" class="btn btn-neutral" title="5. Tests of Association" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Jake Teo.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>