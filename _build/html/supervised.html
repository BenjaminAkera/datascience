

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>6. Supervised Learning &mdash; Data Science 0.1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="Data Science 0.1 documentation" href="index.html"/>
        <link rel="next" title="7. Unsupervised Learning" href="unsupervised.html"/>
        <link rel="prev" title="5. Tests of Association" href="association.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Data Science
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="general.html">1. General Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="assumptions.html">2. Tests for Assumptions</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">3. In-Built Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="difference.html">4. Tests of Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="association.html">5. Tests of Association</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="">6. Supervised Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#classification">6.1. Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-nearest-neighbours-knn">6.1.1. K Nearest Neighbours (KNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#decision-tree">6.1.2. Decision Tree</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-forest">6.1.3. Random Forest</a></li>
<li class="toctree-l3"><a class="reference internal" href="#logistic-regression">6.1.4. Logistic Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#support-vector-machine">6.1.5. Support Vector Machine</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#regression">6.2. Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ols-regression">6.2.1. OLS Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ridge-regression">6.2.2. Ridge Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lasso-regression">6.2.3. LASSO Regression</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="unsupervised.html">7. Unsupervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="decomposition.html">8. Time Series Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting.html">9. Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="resources.html">10. Resources</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">Data Science</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>6. Supervised Learning</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/supervised.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="supervised-learning">
<h1>6. Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="classification">
<h2>6.1. Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="k-nearest-neighbours-knn">
<h3>6.1.1. K Nearest Neighbours (KNN)<a class="headerlink" href="#k-nearest-neighbours-knn" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ol class="last arabic simple">
<li><strong>Distance Metric</strong>: Eclidean Distance (default). In sklearn it is known as (Minkowski with p = 2)</li>
<li><strong>How many nearest neighbour</strong>: k=1 very specific, k=5 more general model. Use nearest k data points to determine classification</li>
<li><strong>Weighting function on neighbours</strong>: (optional)</li>
<li><strong>How to aggregate class of neighbour points</strong>: Simple majority (default)</li>
</ol>
</div>
<div class="code python highlight-python"><div class="highlight"><pre><span></span>#### IMPORT MODULES ####
import pandas as pd
import numpy as np
from sklearn.cross_validation import train_test_split
from sklearn.neighbors import KNeighborsClassifier



#### TRAIN TEST SPLIT ####
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)



#### CREATE MODEL ####
knn = KNeighborsClassifier(n_neighbors = 5)



#### FIT MODEL ####
knn.fit(X_train, y_train)
#KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,
#     metric_params=None, n_jobs=1, n_neighbors=5, p=2,
#     weights=&#39;uniform&#39;)



#### TEST MODEL ####
knn.score(X_test, y_test)
&gt;&gt;&gt; 0.53333333333333333
</pre></div>
</div>
</div>
<div class="section" id="decision-tree">
<h3>6.1.2. Decision Tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">¶</a></h3>
<p>Uses gini index to split the data at binary level.</p>
<p><strong>Strengths:</strong> Can select a large number of features that best determine the targets.
<strong>Weakness:</strong> Tends to overfit the data as it will split till the end.
Pruning can be done to remove the leaves to prevent overfitting but that is not available in sklearn.
Small changes in data can lead to different splits. Not very reproducible for future data (see random forest).</p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span>###### IMPORT MODULES #### ###
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier



#### TRAIN TEST SPLIT ####
train_predictor, test_predictor, train_target, test_target = \
train_test_split(predictor, target, test_size=0.25)

&gt;&gt;&gt; print test_predictor.shape
&gt;&gt;&gt; print train_predictor.shape
(38, 4)
(112, 4)



#### CREATE MODEL ####
clf = DecisionTreeClassifier()



#### FIT MODEL ####
model = clf.fit(train_predictor, train_target)
&gt;&gt;&gt; print model
&gt;&gt;&gt; DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=None, splitter=&#39;best&#39;)



#### TEST MODEL ####
predictions = model.predict(test_predictor)

&gt;&gt;&gt; print sklearn.metrics.confusion_matrix(test_target,predictions)
&gt;&gt;&gt; print sklearn.metrics.accuracy_score(test_target, predictions)*100, &#39;%&#39;
[[14  0  0]
 [ 0 13  0]
 [ 0  1 10]]
97.3684210526 %


#### SCORE MODEL ####
# it is easier to use this package that does everything nicely for a perfect confusion matrix
from pandas_confusion import ConfusionMatrix
&gt;&gt;&gt; ConfusionMatrix(test_target, predictions)
Predicted   setosa  versicolor  virginica  __all__
Actual
setosa          14           0          0       14
versicolor       0          13          0       13
virginica        0           1         10       11
__all__         14          14         10       38



####### FEATURE IMPORTANCE #### ####
df2= pd.DataFrame(model.feature_importances_, index=df.columns[:-2])

&gt;&gt;&gt; df2.sort_values(by=0,ascending=False)
petal width (cm)      0.952542
petal length (cm)     0.029591
sepal length (cm)     0.017867
sepal width (cm)      0.000000
</pre></div>
</div>
</div>
<div class="section" id="random-forest">
<h3>6.1.3. Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h3>
<p>An ensemble of decision trees.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span>###### IMPORT MODULES #### ###
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import train_test_split
import sklearn.metrics



#### TRAIN TEST SPLIT ####
train_feature, test_feature, train_target, test_target = \
train_test_split(feature, target, test_size=0.2)

&gt;&gt;&gt; print train_feature.shape
&gt;&gt;&gt; print test_feature.shape
(404, 13)
(102, 13)


#### CREATE MODEL ####
# use 100 decision trees
clf = RandomForestClassifier(n_estimators=100)



#### FIT MODEL ####
model = clf.fit(train_feature, train_target)
&gt;&gt;&gt; print model
RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)



#### TEST MODEL ####
predictions = model.predict(test_feature)



#### SCORE MODEL ####
&gt;&gt;&gt; print &#39;accuracy&#39;, &#39;\n&#39;, sklearn.metrics.accuracy_score(test_target, predictions)*100, &#39;%&#39;, &#39;\n&#39;
&gt;&gt;&gt; print &#39;confusion matrix&#39;, &#39;\n&#39;, sklearn.metrics.confusion_matrix(test_target,predictions)
accuracy
82.3529411765 %
confusion matrix
[[21  0  3]
 [ 0 21  4]
 [ 8  3 42]]



####### FEATURE IMPORTANCE #### ####
# rank the importance of features
df2= pd.DataFrame(model.feature_importances_, index=df.columns[:-2])
&gt;&gt;&gt; df2.sort_values(by=0,ascending=False)
RM    0.225612
LSTAT 0.192478
CRIM  0.108510
DIS   0.088056
AGE   0.074202
NOX   0.067718
B     0.057706
PTRATIO       0.051702
TAX   0.047568
INDUS 0.037871
RAD   0.026538
ZN    0.012635
CHAS  0.009405



#### GRAPHS ####

# see how many decision trees are minimally required make the accuarcy consistent
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline

trees=range(100)
accuracy=np.zeros(100)

for i in range(len(trees)):
  clf=RandomForestClassifier(n_estimators= i+1)
  model=clf.fit(train_feature, train_target)
  predictions=model.predict(test_feature)
  accuracy[i]=sklearn.metrics.accuracy_score(test_target, predictions)

plt.plot(trees,accuracy)

# well, seems like more than 10 trees will have a consistent accuracy of 0.82.
# Guess there&#39;s no need to have an ensemble of 100 trees!
</pre></div>
</div>
<img alt="_images/randomforest.png" src="_images/randomforest.png" />
</div>
<div class="section" id="logistic-regression">
<h3>6.1.4. Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h3>
<p>Binary output.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span>#### IMPORT MODULES ####
import pandas as pd
import statsmodels.api as sm



#### FIT MODEL ####
lreg = sm.Logit(df3[&#39;diameter_cut&#39;], df3[trainC]).fit()
print lreg.summary()


Optimization terminated successfully.
       Current function value: 0.518121
       Iterations 6
                           Logit Regression Results
==============================================================================
Dep. Variable:           diameter_cut   No. Observations:                18067
Model:                          Logit   Df Residuals:                    18065
Method:                           MLE   Df Model:                            1
Date:                Thu, 04 Aug 2016   Pseudo R-squ.:                  0.2525
Time:                        14:13:14   Log-Likelihood:                -9360.9
converged:                       True   LL-Null:                       -12523.
                                        LLR p-value:                     0.000
================================================================================
                   coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
--------------------------------------------------------------------------------
depth            4.2529      0.067     63.250      0.000         4.121     4.385
layers_YESNO    -2.1102      0.037    -57.679      0.000        -2.182    -2.039
================================================================================



#### CONFIDENCE INTERVALS ####
params = lreg.params
conf = lreg.conf_int()
conf[&#39;OR&#39;] = params
conf.columns = [&#39;Lower CI&#39;, &#39;Upper CI&#39;, &#39;OR&#39;]
print (np.exp(conf))

Lower CI   Upper CI         OR
depth         61.625434  80.209893  70.306255
layers_YESNO   0.112824   0.130223   0.121212
</pre></div>
</div>
</div>
<div class="section" id="support-vector-machine">
<h3>6.1.5. Support Vector Machine<a class="headerlink" href="#support-vector-machine" title="Permalink to this headline">¶</a></h3>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
</div>
<div class="section" id="regression">
<h2>6.2. Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h2>
<div class="section" id="ols-regression">
<h3>6.2.1. OLS Regression<a class="headerlink" href="#ols-regression" title="Permalink to this headline">¶</a></h3>
<p>Ordinary Least Squares Regression or OLS Regression is the most basic form and fundamental of regression.
Best fit line <code class="docutils literal"><span class="pre">ŷ</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">bx</span></code> is drawn based on the ordinary least squares method. i.e., least total area of squares (sum of squares) with length from each x,y point to regresson line.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span>model = smf.ols(formula=&#39;diameter ~ depth&#39;, data=df3).fit()
print model.summary()



OLS Regression Results
==============================================================================
Dep. Variable:               diameter   R-squared:                       0.512
Model:                            OLS   Adj. R-squared:                  0.512
Method:                 Least Squares   F-statistic:                 1.895e+04
Date:                Tue, 02 Aug 2016   Prob (F-statistic):               0.00
Time:                        17:10:34   Log-Likelihood:                -51812.
No. Observations:               18067   AIC:                         1.036e+05
Df Residuals:                   18065   BIC:                         1.036e+05
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept      2.2523      0.054     41.656      0.000         2.146     2.358
depth         11.5836      0.084    137.675      0.000        11.419    11.749
==============================================================================
Omnibus:                    12117.030   Durbin-Watson:                   0.673
Prob(Omnibus):                  0.000   Jarque-Bera (JB):           391356.565
Skew:                           2.771   Prob(JB):                         0.00
Kurtosis:                      25.117   Cond. No.                         3.46
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
<div class="section" id="ridge-regression">
<h3>6.2.2. Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">¶</a></h3>
<p><strong>Regularisaton</strong> is an important concept used in Ridge Regression as well as the next LASSO regression.
Ridge regression uses regularisation which adds a penalty parameter to a variable when it has a large variation.
Regularisation prevents overfitting by restricting the model, thus lowering its complexity.</p>
<blockquote>
<div><ul class="simple">
<li>Uses L2 regularisation, which <em>reduces the sum of squares</em> of the parameters</li>
<li>The influence of L2 is controlled by an alpha parameter. Default is 1.</li>
<li>High alpha means more regularisation and a simpler model.</li>
</ul>
</div></blockquote>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="c1">#### IMPORT MODULES ####</span>
<span class="kn">import</span> <span class="nn">panda</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="c1">#### TRAIN-TEST SPLIT ####</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_crime</span><span class="p">,</span> <span class="n">y_crime</span><span class="p">,</span>
                                                 <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1">#### NORMALIZATION ####</span>
  <span class="c1"># using minmaxscaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>


<span class="c1">#### CREATE AND FIT MODEL ####</span>
<span class="n">linridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">20.0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Crime dataset&#39;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;ridge regression linear model intercept: {}&#39;</span>
   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;ridge regression linear model coeff:</span><span class="se">\n</span><span class="s1">{}&#39;</span>
   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;R-squared score (training): {:.3f}&#39;</span>
   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;R-squared score (test): {:.3f}&#39;</span>
   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Number of non-zero features: {}&#39;</span>
   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
<p>To investigate the effect of alpha:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="s1">&#39;Ridge regression: effect of alpha regularization parameter</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">this_alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]:</span>
  <span class="n">linridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">this_alpha</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">r2_train</span> <span class="o">=</span> <span class="n">linridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">r2_test</span> <span class="o">=</span> <span class="n">linridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
  <span class="n">num_coeff_bigger</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Alpha = {:.2f}</span><span class="se">\n</span><span class="s1">num abs(coeff) &gt; 1.0: {}, </span><span class="se">\</span>
<span class="s1">        r-squared training: {:.2f}, r-squared test: {:.2f}</span><span class="se">\n</span><span class="s1">&#39;</span>
       <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">this_alpha</span><span class="p">,</span> <span class="n">num_coeff_bigger</span><span class="p">,</span> <span class="n">r2_train</span><span class="p">,</span> <span class="n">r2_test</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li>Many variables with small/medium effects: Ridge</li>
<li>Only a few variables with medium/large effects: LASSO</li>
</ul>
</div>
</div>
<div class="section" id="lasso-regression">
<h3>6.2.3. LASSO Regression<a class="headerlink" href="#lasso-regression" title="Permalink to this headline">¶</a></h3>
<p>LASSO refers to Least Absolute Shrinkage and Selection Operator Regression.
Like Ridge Regression this also has a regularisation property.</p>
<ul class="simple">
<li>Uses L1 regularisation, which <em>reduces sum of the absolute values of coefficients</em>, that change unimportant features (their regression coefficients) into 0</li>
<li>The influence of L1 is controlled by an alpha parameter. Default is 1.</li>
<li>High alpha means more regularisation and a simpler model. When alpha = 0, then it is a normal OLS regression.</li>
</ul>
<ol class="loweralpha simple">
<li>Bias increase &amp; variability decreases when alpha increases.</li>
<li>Useful when there are many features (explanatory variables).</li>
<li>Have to standardize all features so that they have mean 0 and std error 1.</li>
<li>Have several algorithms: LAR (Least Angle Regression). Starts w 0 predictors &amp; add each predictor that is most correlated at each step.</li>
</ol>
<div class="code python highlight-python"><div class="highlight"><pre><span></span>#### IMPORT MODULES ####
import pandas as pd
import numpy as py
from sklearn import preprocessing
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import LassoLarsCV
import sklearn.metrics
from sklearn.datasets import load_boston



#### NORMALIZATION ####
# standardise the means to 0 and standard error to 1
for i in df.columns[:-1]: # df.columns[:-1] = dataframe for all features
  df[i] = preprocessing.scale(df[i].astype(&#39;float64&#39;))
&gt;&gt;&gt; df.describe()



#### TRAIN TEST SPLIT ####
train_feature, test_feature, train_target, test_target = \
train_test_split(feature, target, random_state=123, test_size=0.2)

&gt;&gt;&gt; print train_feature.shape
&gt;&gt;&gt; print test_feature.shape
&gt;&gt;&gt; (404, 13)
&gt;&gt;&gt; (102, 13)



#### CREATE MODEL ####
# Fit the LASSO LAR regression model
# cv=10; use k-fold cross validation
# precompute; True=model will be faster if dataset is large
model=LassoLarsCV(cv=10, precompute=False)



#### FIT MODEL ####
model = model.fit(train_feature,train_target)
&gt;&gt;&gt; print model
LassoLarsCV(copy_X=True, cv=10, eps=2.2204460492503131e-16,
      fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,
      normalize=True, positive=False, precompute=False, verbose=False)



#### ANALYSE COEFFICIENTS ####
Compare the regression coefficients, and see which one LASSO removed.
LSTAT is the most important predictor, followed by RM, DIS, and RAD. AGE is removed by LASSO

&gt;&gt;&gt; df2=pd.DataFrame(model.coef_, index=feature.columns)
&gt;&gt;&gt; df2.sort_values(by=0,ascending=False)
RM    3.050843
RAD   2.040252
ZN    1.004318
B     0.629933
CHAS  0.317948
INDUS 0.225688
AGE   0.000000
CRIM  -0.770291
NOX   -1.617137
TAX   -1.731576
PTRATIO       -1.923485
DIS   -2.733660
LSTAT -3.878356



#### SCORE MODEL ####
# MSE from training and test data
from sklearn.metrics import mean_squared_error
train_error = mean_squared_error(train_target, model.predict(train_feature))
test_error = mean_squared_error(test_target, model.predict(test_feature))

print (&#39;training data MSE&#39;)
print(train_error)
print (&#39;test data MSE&#39;)
print(test_error)

# MSE closer to 0 are better
# test dataset is less accurate as expected
&gt;&gt;&gt; training data MSE
&gt;&gt;&gt; 20.7279948891
&gt;&gt;&gt; test data MSE
&gt;&gt;&gt; 28.3767672242


# R-square from training and test data
rsquared_train=model.score(train_feature,train_target)
rsquared_test=model.score(test_feature,test_target)
print (&#39;training data R-square&#39;)
print(rsquared_train)
print (&#39;test data R-square&#39;)
print(rsquared_test)

# test data explained 65% of the predictors
&gt;&gt;&gt; training data R-square
&gt;&gt;&gt; 0.755337444405
&gt;&gt;&gt; test data R-square
&gt;&gt;&gt; 0.657019301268
</pre></div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="unsupervised.html" class="btn btn-neutral float-right" title="7. Unsupervised Learning" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="association.html" class="btn btn-neutral" title="5. Tests of Association" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Jake Teo.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>