

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>4. Supervised Learning &mdash; Data Science 0.1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="Data Science 0.1 documentation" href="index.html"/>
        <link rel="next" title="5. Unsupervised Learning" href="unsupervised.html"/>
        <link rel="prev" title="3. Tests of Association" href="association.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Data Science
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="general.html">1. General Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="difference.html">2. Tests of Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="association.html">3. Tests of Association</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="">4. Supervised Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#classification">4.1. Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-nearest-neighbours-knn">4.1.1. K Nearest Neighbours (KNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#decision-tree">4.1.2. Decision Tree</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-forest">4.1.3. Random Forest</a></li>
<li class="toctree-l3"><a class="reference internal" href="#logistic-regression">4.1.4. Logistic Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#support-vector-machine">4.1.5. Support Vector Machine</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#regression">4.2. Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ordinary-least-squares-ols-regression">4.2.1. Ordinary Least Squares (OLS) Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ridge-regression">4.2.2. Ridge Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lasso-regression">4.2.3. Lasso Regression</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="unsupervised.html">5. Unsupervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="decomposition.html">6. Time Series Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting.html">7. Forecasting</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">Data Science</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>4. Supervised Learning</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/supervised.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="supervised-learning">
<h1>4. Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="classification">
<h2>4.1. Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="k-nearest-neighbours-knn">
<h3>4.1.1. K Nearest Neighbours (KNN)<a class="headerlink" href="#k-nearest-neighbours-knn" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><code class="docutils literal"><span class="pre">Distance</span> <span class="pre">Metric:</span></code> Eclidean Distance (default). In sklearn it is known as (Minkowski with p = 2)</li>
<li><code class="docutils literal"><span class="pre">How</span> <span class="pre">many</span> <span class="pre">nearest</span> <span class="pre">neighbour</span> <span class="pre">to</span> <span class="pre">look</span> <span class="pre">at:</span></code> k=1 very specific, k=5 more general model. Use nearest k data points to determine classification</li>
<li><code class="docutils literal"><span class="pre">Weighting</span> <span class="pre">function</span> <span class="pre">on</span> <span class="pre">neighbours:</span></code> (optional)</li>
<li><code class="docutils literal"><span class="pre">How</span> <span class="pre">to</span> <span class="pre">aggregate</span> <span class="pre">class</span> <span class="pre">of</span> <span class="pre">neighbour</span> <span class="pre">points:</span></code> Simple majority (default)</li>
</ol>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Import Modules</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="c1"># Train Test Split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Create Model</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1">#Fit Model</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1">#KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,</span>
<span class="c1">#     metric_params=None, n_jobs=1, n_neighbors=5, p=2,</span>
<span class="c1">#     weights=&#39;uniform&#39;)</span>

<span class="c1">#Test Module</span>
<span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="c1">#0.53333333333333333</span>
</pre></div>
</div>
</div>
<div class="section" id="decision-tree">
<h3>4.1.2. Decision Tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">¶</a></h3>
<p>Uses gini index to split the data at binary level.</p>
<p><strong>Strengths:</strong> Can select a large number of features that best determine the targets.
<strong>Weakness:</strong> Tends to overfit the data as it will split till the end.
Pruning can be done to remove the leaves to prevent overfitting but that is not available in sklearn.
Small changes in data can lead to different splits. Not very reproducible for future data (see random forest).</p>
<p><strong>Train Test Split</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="n">train_predictor</span><span class="p">,</span> <span class="n">test_predictor</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">predictor</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">test_predictor</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">train_predictor</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(38, 4)</span>
<span class="go">(112, 4)</span>
</pre></div>
</div>
<p><strong>Create Model</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Fit Model</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_predictor</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">model</span>
<span class="go">DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None,</span>
<span class="go">            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,</span>
<span class="go">            min_samples_split=2, min_weight_fraction_leaf=0.0,</span>
<span class="go">            presort=False, random_state=None, splitter=&#39;best&#39;)</span>
</pre></div>
</div>
<p><strong>Test Model</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_predictor</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Score Model</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span><span class="n">predictions</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;%&#39;</span>
<span class="go">[[14  0  0]</span>
<span class="go"> [ 0 13  0]</span>
<span class="go"> [ 0  1 10]]</span>
<span class="go">97.3684210526 %</span>
</pre></div>
</div>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="c1"># it is easier to use this package that does everything nicely for a perfect confusion matrix</span>
<span class="kn">from</span> <span class="nn">pandas_confusion</span> <span class="kn">import</span> <span class="n">ConfusionMatrix</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ConfusionMatrix</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="go">Predicted   setosa  versicolor  virginica  __all__</span>
<span class="go">Actual</span>
<span class="go">setosa          14           0          0       14</span>
<span class="go">versicolor       0          13          0       13</span>
<span class="go">virginica        0           1         10       11</span>
<span class="go">__all__         14          14         10       38</span>
</pre></div>
</div>
<p><strong>Feature Importance</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="n">df2</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="go">petal width (cm)        0.952542</span>
<span class="go">petal length (cm)       0.029591</span>
<span class="go">sepal length (cm)       0.017867</span>
<span class="go">sepal width (cm)        0.000000</span>
</pre></div>
</div>
</div>
<div class="section" id="random-forest">
<h3>4.1.3. Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h3>
<p>An ensemble of decision trees.</p>
<p><strong>Import Modules</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span>
</pre></div>
</div>
<p><strong>Train Test Split</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="n">train_feature</span><span class="p">,</span> <span class="n">test_feature</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> \
<span class="n">train_test_split</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">train_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">test_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(404, 13)</span>
<span class="go">(102, 13)</span>
</pre></div>
</div>
<p><strong>Create Model</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="c1"># use 100 decision trees</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Fit Model</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_feature</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">model</span>
<span class="go">RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,</span>
<span class="go">            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,</span>
<span class="go">            min_samples_leaf=1, min_samples_split=2,</span>
<span class="go">            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,</span>
<span class="go">            oob_score=False, random_state=None, verbose=0,</span>
<span class="go">            warm_start=False)</span>
</pre></div>
</div>
<p><strong>Test Model</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_feature</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Score Model</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;%&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s1">&#39;confusion matrix&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span><span class="n">predictions</span><span class="p">)</span>
<span class="go">accuracy</span>
<span class="go">82.3529411765 %</span>
<span class="go">confusion matrix</span>
<span class="go">[[21  0  3]</span>
<span class="go"> [ 0 21  4]</span>
<span class="go"> [ 8  3 42]]</span>
</pre></div>
</div>
<p><strong>Feature Importance</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="c1"># rank the importance of features</span>
<span class="n">df2</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="go"> RM     0.225612</span>
<span class="go"> LSTAT  0.192478</span>
<span class="go"> CRIM   0.108510</span>
<span class="go"> DIS    0.088056</span>
<span class="go"> AGE    0.074202</span>
<span class="go"> NOX    0.067718</span>
<span class="go"> B      0.057706</span>
<span class="go"> PTRATIO        0.051702</span>
<span class="go"> TAX    0.047568</span>
<span class="go"> INDUS  0.037871</span>
<span class="go"> RAD    0.026538</span>
<span class="go"> ZN     0.012635</span>
<span class="go"> CHAS   0.009405</span>
</pre></div>
</div>
<p><strong>Optimum Ensemble of Trees</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span># see how many decision trees are minimally required make the accuarcy consistent
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline

trees=range(100)
accuracy=np.zeros(100)

for i in range(len(trees)):
   clf=RandomForestClassifier(n_estimators= i+1)
   model=clf.fit(train_feature, train_target)
   predictions=model.predict(test_feature)
   accuracy[i]=sklearn.metrics.accuracy_score(test_target, predictions)

plt.plot(trees,accuracy)

# well, seems like more than 10 trees will have a consistent accuracy of 0.82.
# Guess there&#39;s no need to have an ensemble of 100 trees!
</pre></div>
</div>
<img alt="_images/randomforest.png" src="_images/randomforest.png" />
</div>
<div class="section" id="logistic-regression">
<h3>4.1.4. Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="support-vector-machine">
<h3>4.1.5. Support Vector Machine<a class="headerlink" href="#support-vector-machine" title="Permalink to this headline">¶</a></h3>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
</div>
<div class="section" id="regression">
<h2>4.2. Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h2>
<div class="section" id="ordinary-least-squares-ols-regression">
<h3>4.2.1. Ordinary Least Squares (OLS) Regression<a class="headerlink" href="#ordinary-least-squares-ols-regression" title="Permalink to this headline">¶</a></h3>
<p>Best fit line <code class="docutils literal"><span class="pre">ŷ</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">bx</span></code> is drawn based on the ordinary least squares method. i.e., least total area of squares with length from each x,y point to regresson line.</p>
</div>
<div class="section" id="ridge-regression">
<h3>4.2.2. Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="lasso-regression">
<h3>4.2.3. Lasso Regression<a class="headerlink" href="#lasso-regression" title="Permalink to this headline">¶</a></h3>
<p>Least absolute shrinkage and selection operator regression, or LASSO regression, has a unique penalty parameter, lambda that <em>change unimportant features (their regression coefficients) into 0</em>.
This helps to prevent <em>overfitting</em>.</p>
<ul class="simple">
<li>Prevent overfitting.</li>
<li>Uses regularisation.</li>
<li>Uses a penalty parameter lambda to change unimportant features (their regression coefficients) into 0. When lambda = 0, then it is a normal OLS regression. (Note sklearn name it as alpha instead)<ol class="loweralpha">
<li>Bias increase &amp; variability decreases when lambda increases.</li>
<li>Useful when there are many features (explanatory variables).</li>
<li>Have to standardize all features so that they have mean 0 and std error 1.</li>
<li>Have several algorithms: LAR (Least Angle Regression). Starts w 0 predictors &amp; add each predictor that is most correlated at each step.</li>
</ol>
</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">sklearn define lambda as alpha instead.</p>
</div>
<p><strong>Import Modules</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">py</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoLarsCV</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
</pre></div>
</div>
<p><strong>Normalization</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="c1"># standardise the means to 0 and standard error to 1</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span> <span class="c1"># df.columns[:-1] = dataframe for all features</span>
  <span class="n">df</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float64&#39;</span><span class="p">))</span>

<span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Train Test Split</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="n">train_feature</span><span class="p">,</span> <span class="n">test_feature</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> \
<span class="n">train_test_split</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">train_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">test_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(404, 13)</span>
<span class="go">(102, 13)</span>
</pre></div>
</div>
<p><strong>Create Model</strong></p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span><span class="c1"># Fit the LASSO LAR regression model</span>
<span class="c1"># cv=10; use k-fold cross validation</span>
<span class="c1"># precompute; True=model will be faster if dataset is large</span>
<span class="n">model</span><span class="o">=</span><span class="n">LassoLarsCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">precompute</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Fit Model</strong></p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_feature</span><span class="p">,</span><span class="n">train_target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="n">model</span>
<span class="go">LassoLarsCV(copy_X=True, cv=10, eps=2.2204460492503131e-16,</span>
<span class="go">      fit_intercept=True, max_iter=500, max_n_alphas=1000, n_jobs=1,</span>
<span class="go">      normalize=True, positive=False, precompute=False, verbose=False)</span>
</pre></div>
</div>
<p><strong>Analyse Coefficients</strong></p>
<p>Compare the regression coefficients, and see which one LASSO removed.
LSTAT is the most important predictor, followed by RM, DIS, and RAD. AGE is removed by LASSO</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">feature</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">df2</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="go">RM      3.050843</span>
<span class="go">RAD     2.040252</span>
<span class="go">ZN      1.004318</span>
<span class="go">B       0.629933</span>
<span class="go">CHAS    0.317948</span>
<span class="go">INDUS   0.225688</span>
<span class="go">AGE     0.000000</span>
<span class="go">CRIM    -0.770291</span>
<span class="go">NOX     -1.617137</span>
<span class="go">TAX     -1.731576</span>
<span class="go">PTRATIO -1.923485</span>
<span class="go">DIS     -2.733660</span>
<span class="go">LSTAT   -3.878356</span>
</pre></div>
</div>
<p><strong>Score Model</strong></p>
<p>Mean Square Errors.</p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span># MSE from training and test data
from sklearn.metrics import mean_squared_error
train_error = mean_squared_error(train_target, model.predict(train_feature))
test_error = mean_squared_error(test_target, model.predict(test_feature))

print (&#39;training data MSE&#39;)
print(train_error)
print (&#39;test data MSE&#39;)
print(test_error)

# MSE closer to 0 are better
# test dataset is less accurate as expected
training data MSE
20.7279948891
test data MSE
28.3767672242
</pre></div>
</div>
<p>R-Square</p>
<div class="code python highlight-python"><div class="highlight"><pre><span></span># R-square from training and test data
rsquared_train=model.score(train_feature,train_target)
rsquared_test=model.score(test_feature,test_target)
print (&#39;training data R-square&#39;)
print(rsquared_train)
print (&#39;test data R-square&#39;)
print(rsquared_test)

# test data explained 65% of the predictors
training data R-square
0.755337444405
test data R-square
0.657019301268
</pre></div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="unsupervised.html" class="btn btn-neutral float-right" title="5. Unsupervised Learning" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="association.html" class="btn btn-neutral" title="3. Tests of Association" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Jake Teo.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>