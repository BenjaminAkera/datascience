

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>7. Supervised Learning &mdash; Data Science 0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Data Science 0.1 documentation" href="index.html"/>
        <link rel="next" title="8. Unsupervised Learning" href="unsupervised.html"/>
        <link rel="prev" title="6. Tests of Association" href="association.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Data Science
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="general.html">1. General Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="assumptions.html">2. Tests for Assumptions</a></li>
<li class="toctree-l1"><a class="reference internal" href="datasets.html">3. In-Built Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="normalisation.html">4. Feature Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="difference.html">5. Tests of Difference</a></li>
<li class="toctree-l1"><a class="reference internal" href="association.html">6. Tests of Association</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">7. Supervised Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#classification">7.1. Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-nearest-neighbours-knn">7.1.1. K Nearest Neighbours (KNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#decision-tree">7.1.2. Decision Tree</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-forest">7.1.3. Random Forest</a></li>
<li class="toctree-l3"><a class="reference internal" href="#logistic-regression">7.1.4. Logistic Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#support-vector-machine">7.1.5. Support Vector Machine</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#regression">7.2. Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ols-regression">7.2.1. OLS Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ridge-regression">7.2.2. Ridge Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lasso-regression">7.2.3. LASSO Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#polynomial-regression">7.2.4. Polynomial Regression</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="unsupervised.html">8. Unsupervised Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="decomposition.html">9. Time Series Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="forecasting.html">10. Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="resources.html">11. Resources</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Data Science</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>7. Supervised Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/supervised.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="supervised-learning">
<h1>7. Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="classification">
<h2>7.1. Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="k-nearest-neighbours-knn">
<h3>7.1.1. K Nearest Neighbours (KNN)<a class="headerlink" href="#k-nearest-neighbours-knn" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ol class="last arabic simple">
<li><strong>Distance Metric</strong>: Eclidean Distance (default). In sklearn it is known as (Minkowski with p = 2)</li>
<li><strong>How many nearest neighbour</strong>: k=1 very specific, k=5 more general model. Use nearest k data points to determine classification</li>
<li><strong>Weighting function on neighbours</strong>: (optional)</li>
<li><strong>How to aggregate class of neighbour points</strong>: Simple majority (default)</li>
</ol>
</div>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">#### IMPORT MODULES ####</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">KNeighborsClassifier</span>



<span class="c1">#### TRAIN TEST SPLIT ####</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>



<span class="c1">#### CREATE MODEL ####</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>



<span class="c1">#### FIT MODEL ####</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1">#KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,</span>
<span class="c1">#     metric_params=None, n_jobs=1, n_neighbors=5, p=2,</span>
<span class="c1">#     weights=&#39;uniform&#39;)</span>



<span class="c1">#### TEST MODEL ####</span>
<span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">0.53333333333333333</span>
</pre></div>
</div>
</div>
<div class="section" id="decision-tree">
<h3>7.1.2. Decision Tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">¶</a></h3>
<p>Uses gini index to split the data at binary level.</p>
<p><strong>Strengths:</strong> Can select a large number of features that best determine the targets.
<strong>Weakness:</strong> Tends to overfit the data as it will split till the end.
Pruning can be done to remove the leaves to prevent overfitting but that is not available in sklearn.
Small changes in data can lead to different splits. Not very reproducible for future data (see random forest).</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">###### IMPORT MODULES #### ###</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>



<span class="c1">#### TRAIN TEST SPLIT ####</span>
<span class="n">train_predictor</span><span class="p">,</span> <span class="n">test_predictor</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> \
<span class="n">train_test_split</span><span class="p">(</span><span class="n">predictor</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">test_predictor</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">train_predictor</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">38</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="p">(</span><span class="mi">112</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>



<span class="c1">#### CREATE MODEL ####</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>



<span class="c1">#### FIT MODEL ####</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_predictor</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">model</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">presort</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">splitter</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>



<span class="c1">#### TEST MODEL ####</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_predictor</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span><span class="n">predictions</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;%&#39;</span>
<span class="p">[[</span><span class="mi">14</span>  <span class="mi">0</span>  <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">0</span> <span class="mi">13</span>  <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">0</span>  <span class="mi">1</span> <span class="mi">10</span><span class="p">]]</span>
<span class="mf">97.3684210526</span> <span class="o">%</span>


<span class="c1">#### SCORE MODEL ####</span>
<span class="c1"># it is easier to use this package that does everything nicely for a perfect confusion matrix</span>
<span class="kn">from</span> <span class="nn">pandas_confusion</span> <span class="k">import</span> <span class="n">ConfusionMatrix</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">ConfusionMatrix</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="n">Predicted</span>   <span class="n">setosa</span>  <span class="n">versicolor</span>  <span class="n">virginica</span>  <span class="n">__all__</span>
<span class="n">Actual</span>
<span class="n">setosa</span>          <span class="mi">14</span>           <span class="mi">0</span>          <span class="mi">0</span>       <span class="mi">14</span>
<span class="n">versicolor</span>       <span class="mi">0</span>          <span class="mi">13</span>          <span class="mi">0</span>       <span class="mi">13</span>
<span class="n">virginica</span>        <span class="mi">0</span>           <span class="mi">1</span>         <span class="mi">10</span>       <span class="mi">11</span>
<span class="n">__all__</span>         <span class="mi">14</span>          <span class="mi">14</span>         <span class="mi">10</span>       <span class="mi">38</span>



<span class="c1">####### FEATURE IMPORTANCE #### ####</span>
<span class="n">df2</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">df2</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">petal</span> <span class="n">width</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>      <span class="mf">0.952542</span>
<span class="n">petal</span> <span class="n">length</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>     <span class="mf">0.029591</span>
<span class="n">sepal</span> <span class="n">length</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>     <span class="mf">0.017867</span>
<span class="n">sepal</span> <span class="n">width</span> <span class="p">(</span><span class="n">cm</span><span class="p">)</span>      <span class="mf">0.000000</span>
</pre></div>
</div>
</div>
<div class="section" id="random-forest">
<h3>7.1.3. Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h3>
<p>An ensemble of decision trees.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">###### IMPORT MODULES #### ###</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span>



<span class="c1">#### TRAIN TEST SPLIT ####</span>
<span class="n">train_feature</span><span class="p">,</span> <span class="n">test_feature</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> \
<span class="n">train_test_split</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">train_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">test_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">404</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>
<span class="p">(</span><span class="mi">102</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>


<span class="c1">#### CREATE MODEL ####</span>
<span class="c1"># use 100 decision trees</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>



<span class="c1">#### FIT MODEL ####</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_feature</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">model</span>
<span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span>
            <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">oob_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>



<span class="c1">#### TEST MODEL ####</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_feature</span><span class="p">)</span>



<span class="c1">#### SCORE MODEL ####</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="s1">&#39;%&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s1">&#39;confusion matrix&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">accuracy</span>
<span class="mf">82.3529411765</span> <span class="o">%</span>
<span class="n">confusion</span> <span class="n">matrix</span>
<span class="p">[[</span><span class="mi">21</span>  <span class="mi">0</span>  <span class="mi">3</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">0</span> <span class="mi">21</span>  <span class="mi">4</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">8</span>  <span class="mi">3</span> <span class="mi">42</span><span class="p">]]</span>



<span class="c1">####### FEATURE IMPORTANCE #### ####</span>
<span class="c1"># rank the importance of features</span>
<span class="n">df2</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">df2</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">RM</span>    <span class="mf">0.225612</span>
<span class="n">LSTAT</span> <span class="mf">0.192478</span>
<span class="n">CRIM</span>  <span class="mf">0.108510</span>
<span class="n">DIS</span>   <span class="mf">0.088056</span>
<span class="n">AGE</span>   <span class="mf">0.074202</span>
<span class="n">NOX</span>   <span class="mf">0.067718</span>
<span class="n">B</span>     <span class="mf">0.057706</span>
<span class="n">PTRATIO</span>       <span class="mf">0.051702</span>
<span class="n">TAX</span>   <span class="mf">0.047568</span>
<span class="n">INDUS</span> <span class="mf">0.037871</span>
<span class="n">RAD</span>   <span class="mf">0.026538</span>
<span class="n">ZN</span>    <span class="mf">0.012635</span>
<span class="n">CHAS</span>  <span class="mf">0.009405</span>



<span class="c1">#### GRAPHS ####</span>

<span class="c1"># see how many decision trees are minimally required make the accuarcy consistent</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">trees</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">accuracy</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trees</span><span class="p">)):</span>
  <span class="n">clf</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">model</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_feature</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
  <span class="n">predictions</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_feature</span><span class="p">)</span>
  <span class="n">accuracy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trees</span><span class="p">,</span><span class="n">accuracy</span><span class="p">)</span>

<span class="c1"># well, seems like more than 10 trees will have a consistent accuracy of 0.82.</span>
<span class="c1"># Guess there&#39;s no need to have an ensemble of 100 trees!</span>
</pre></div>
</div>
<img alt="_images/randomforest.png" src="_images/randomforest.png" />
</div>
<div class="section" id="logistic-regression">
<h3>7.1.4. Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h3>
<p>Binary output or y value.</p>
<img alt="_images/logisticR.png" src="_images/logisticR.png" />
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">#### IMPORT MODULES ####</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>



<span class="c1">#### FIT MODEL ####</span>
<span class="n">lreg</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">Logit</span><span class="p">(</span><span class="n">df3</span><span class="p">[</span><span class="s1">&#39;diameter_cut&#39;</span><span class="p">],</span> <span class="n">df3</span><span class="p">[</span><span class="n">trainC</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span> <span class="n">lreg</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>


<span class="n">Optimization</span> <span class="n">terminated</span> <span class="n">successfully</span><span class="o">.</span>
       <span class="n">Current</span> <span class="n">function</span> <span class="n">value</span><span class="p">:</span> <span class="mf">0.518121</span>
       <span class="n">Iterations</span> <span class="mi">6</span>
                           <span class="n">Logit</span> <span class="n">Regression</span> <span class="n">Results</span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span> <span class="n">Variable</span><span class="p">:</span>           <span class="n">diameter_cut</span>   <span class="n">No</span><span class="o">.</span> <span class="n">Observations</span><span class="p">:</span>                <span class="mi">18067</span>
<span class="n">Model</span><span class="p">:</span>                          <span class="n">Logit</span>   <span class="n">Df</span> <span class="n">Residuals</span><span class="p">:</span>                    <span class="mi">18065</span>
<span class="n">Method</span><span class="p">:</span>                           <span class="n">MLE</span>   <span class="n">Df</span> <span class="n">Model</span><span class="p">:</span>                            <span class="mi">1</span>
<span class="n">Date</span><span class="p">:</span>                <span class="n">Thu</span><span class="p">,</span> <span class="mi">04</span> <span class="n">Aug</span> <span class="mi">2016</span>   <span class="n">Pseudo</span> <span class="n">R</span><span class="o">-</span><span class="n">squ</span><span class="o">.</span><span class="p">:</span>                  <span class="mf">0.2525</span>
<span class="n">Time</span><span class="p">:</span>                        <span class="mi">14</span><span class="p">:</span><span class="mi">13</span><span class="p">:</span><span class="mi">14</span>   <span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span>                <span class="o">-</span><span class="mf">9360.9</span>
<span class="n">converged</span><span class="p">:</span>                       <span class="kc">True</span>   <span class="n">LL</span><span class="o">-</span><span class="n">Null</span><span class="p">:</span>                       <span class="o">-</span><span class="mf">12523.</span>
                                        <span class="n">LLR</span> <span class="n">p</span><span class="o">-</span><span class="n">value</span><span class="p">:</span>                     <span class="mf">0.000</span>
<span class="o">================================================================================</span>
                   <span class="n">coef</span>    <span class="n">std</span> <span class="n">err</span>          <span class="n">z</span>      <span class="n">P</span><span class="o">&gt;|</span><span class="n">z</span><span class="o">|</span>      <span class="p">[</span><span class="mf">95.0</span><span class="o">%</span> <span class="n">Conf</span><span class="o">.</span> <span class="n">Int</span><span class="o">.</span><span class="p">]</span>
<span class="o">--------------------------------------------------------------------------------</span>
<span class="n">depth</span>            <span class="mf">4.2529</span>      <span class="mf">0.067</span>     <span class="mf">63.250</span>      <span class="mf">0.000</span>         <span class="mf">4.121</span>     <span class="mf">4.385</span>
<span class="n">layers_YESNO</span>    <span class="o">-</span><span class="mf">2.1102</span>      <span class="mf">0.037</span>    <span class="o">-</span><span class="mf">57.679</span>      <span class="mf">0.000</span>        <span class="o">-</span><span class="mf">2.182</span>    <span class="o">-</span><span class="mf">2.039</span>
<span class="o">================================================================================</span>



<span class="c1">#### CONFIDENCE INTERVALS ####</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">lreg</span><span class="o">.</span><span class="n">params</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">lreg</span><span class="o">.</span><span class="n">conf_int</span><span class="p">()</span>
<span class="n">conf</span><span class="p">[</span><span class="s1">&#39;OR&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span>
<span class="n">conf</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Lower CI&#39;</span><span class="p">,</span> <span class="s1">&#39;Upper CI&#39;</span><span class="p">,</span> <span class="s1">&#39;OR&#39;</span><span class="p">]</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">conf</span><span class="p">))</span>

<span class="n">Lower</span> <span class="n">CI</span>   <span class="n">Upper</span> <span class="n">CI</span>         <span class="n">OR</span>
<span class="n">depth</span>         <span class="mf">61.625434</span>  <span class="mf">80.209893</span>  <span class="mf">70.306255</span>
<span class="n">layers_YESNO</span>   <span class="mf">0.112824</span>   <span class="mf">0.130223</span>   <span class="mf">0.121212</span>
</pre></div>
</div>
<p>A regularisation penlty L2, just like ridge regression is by default in <code class="docutils literal"><span class="pre">sklearn.linear_model</span></code>,
<code class="docutils literal"><span class="pre">`LogisticRegression</span></code>, controlled using the parameter C (default 1).</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">adspy_shared_utilities</span> <span class="k">import</span> <span class="p">(</span>
<span class="n">plot_class_regions_for_classifier_subplot</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">subaxes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">y_fruits_apple</span> <span class="o">=</span> <span class="n">y_fruits_2d</span> <span class="o">==</span> <span class="mi">1</span>   <span class="c1"># make into a binary problem: apples vs everything else</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="p">(</span>
<span class="n">train_test_split</span><span class="p">(</span><span class="n">X_fruits_2d</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">(),</span>
                <span class="n">y_fruits_apple</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">(),</span>
                <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plot_class_regions_for_classifier_subplot</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                         <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Logistic regression </span><span class="se">\</span>
<span class="s1">for binary classification</span><span class="se">\n</span><span class="s1">Fruit dataset: Apple vs others&#39;</span><span class="p">,</span>
                                         <span class="n">subaxes</span><span class="p">)</span>

<span class="n">h</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">w</span> <span class="o">=</span> <span class="mi">8</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;A fruit with height </span><span class="si">{}</span><span class="s1"> and width </span><span class="si">{}</span><span class="s1"> is predicted to be: </span><span class="si">{}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;not an apple&#39;</span><span class="p">,</span> <span class="s1">&#39;an apple&#39;</span><span class="p">][</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]]))</span>

<span class="n">h</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">w</span> <span class="o">=</span> <span class="mi">7</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;A fruit with height </span><span class="si">{}</span><span class="s1"> and width </span><span class="si">{}</span><span class="s1"> is predicted to be: </span><span class="si">{}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;not an apple&#39;</span><span class="p">,</span> <span class="s1">&#39;an apple&#39;</span><span class="p">][</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]]))</span>
<span class="n">subaxes</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;height&#39;</span><span class="p">)</span>
<span class="n">subaxes</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;width&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy of Logistic regression classifier on training set: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy of Logistic regression classifier on test set: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="section" id="support-vector-machine">
<h3>7.1.5. Support Vector Machine<a class="headerlink" href="#support-vector-machine" title="Permalink to this headline">¶</a></h3>
<p>Have 3 tuning parameters. Need to normalize first too!</p>
<ol class="arabic simple">
<li>Have regularisation using parameter C, just like logistic regression. Default to 1. Limits the importance of each point.</li>
<li>Type of kernel. Default is Radial Basis Function (RBF)</li>
<li>Gamma parameter for adjusting kernel width. Influence of a single training example reaches. Low gamma &gt; far reach, high values &gt; limited reach.</li>
</ol>
<img alt="_images/svm_parameters.PNG" src="_images/svm_parameters.PNG" />
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">adspy_shared_utilities</span> <span class="k">import</span> <span class="n">plot_class_regions_for_classifier_subplot</span>


<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_C2</span><span class="p">,</span> <span class="n">y_C2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">subaxes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">this_C</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">this_C</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Linear SVC, C = </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">this_C</span><span class="p">)</span>
<span class="n">plot_class_regions_for_classifier_subplot</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">subaxes</span><span class="p">)</span>
</pre></div>
</div>
<p>We can directly call a linear SVC by directly importing the <code class="docutils literal"><span class="pre">LinearSVC</span></code> function</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">LinearSVC</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_cancer</span><span class="p">,</span> <span class="n">y_cancer</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Breast cancer dataset&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy of Linear SVC classifier on training set: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy of Linear SVC classifier on test set: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
<p><strong>Multi-Class Classification</strong>, i.e., having more than 2 target values, is also possible.
With the results, it is possible to compare one class versus all other classes.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">LinearSVC</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_fruits_2d</span><span class="p">,</span> <span class="n">y_fruits_2d</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">67</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coefficients:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Intercepts:</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
<p>visualising in a graph...</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">]</span>
<span class="n">cmap_fruits</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#00FF00&#39;</span><span class="p">,</span> <span class="s1">&#39;#0000FF&#39;</span><span class="p">,</span><span class="s1">&#39;#FFFF00&#39;</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_fruits_2d</span><span class="p">[[</span><span class="s1">&#39;height&#39;</span><span class="p">]],</span> <span class="n">X_fruits_2d</span><span class="p">[[</span><span class="s1">&#39;width&#39;</span><span class="p">]],</span>
           <span class="n">c</span><span class="o">=</span><span class="n">y_fruits_2d</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_fruits</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">7</span><span class="p">)</span>

<span class="n">x_0_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>

<span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">]):</span>
    <span class="c1"># Since class prediction with a linear model uses the formula y = w_0 x_0 + w_1 x_1 + b,</span>
    <span class="c1"># and the decision boundary is defined as being all points with y = 0, to plot x_1 as a</span>
    <span class="c1"># function of x_0 we just solve w_0 x_0 + w_1 x_1 + b = 0 for x_1:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_0_range</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="n">x_0_range</span> <span class="o">*</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">target_names_fruits</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;height&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;width&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Kernalised Support Vector Machines</strong></p>
<p>For complex classification, new dimensions can be added to SVM. e.g., square of x.
There are many types of kernal transformations. By default, SVM will use the Radial Basis Function (RBF) kernel.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">adspy_shared_utilities</span> <span class="k">import</span> <span class="n">plot_class_regions_for_classifier</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_D2</span><span class="p">,</span> <span class="n">y_D2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># The default SVC kernel is radial basis function (RBF)</span>
<span class="n">plot_class_regions_for_classifier</span><span class="p">(</span><span class="n">SVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span>
                                 <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="s1">&#39;Support Vector Classifier: RBF kernel&#39;</span><span class="p">)</span>

<span class="c1"># Compare decision boundries with polynomial kernel, degree = 3</span>
<span class="n">plot_class_regions_for_classifier</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">degree</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
                                 <span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="n">X_train</span><span class="p">,</span>
                                 <span class="n">y_train</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="s1">&#39;Support Vector Classifier: Polynomial kernel, degree = 3&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Full tuning in Support Vector Machines, using normalisation, kernel tuning, and regularisation.</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">MinMaxScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamme</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Breast cancer dataset (normalized with MinMax scaling)&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;RBF-kernel SVC (with MinMax scaling) training set accuracy: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;RBF-kernel SVC (with MinMax scaling) test set accuracy: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
</div>
</div>
<div class="section" id="regression">
<h2>7.2. Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h2>
<div class="section" id="ols-regression">
<h3>7.2.1. OLS Regression<a class="headerlink" href="#ols-regression" title="Permalink to this headline">¶</a></h3>
<p>Ordinary Least Squares Regression or OLS Regression is the most basic form and fundamental of regression.
Best fit line <code class="docutils literal"><span class="pre">ŷ</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">bx</span></code> is drawn based on the ordinary least squares method. i.e., least total area of squares (sum of squares) with length from each x,y point to regresson line.</p>
<p>OLS can be conducted using statsmodel package...</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s1">&#39;diameter ~ depth&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span> <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>



<span class="n">OLS</span> <span class="n">Regression</span> <span class="n">Results</span>
<span class="o">==============================================================================</span>
<span class="n">Dep</span><span class="o">.</span> <span class="n">Variable</span><span class="p">:</span>               <span class="n">diameter</span>   <span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span>                       <span class="mf">0.512</span>
<span class="n">Model</span><span class="p">:</span>                            <span class="n">OLS</span>   <span class="n">Adj</span><span class="o">.</span> <span class="n">R</span><span class="o">-</span><span class="n">squared</span><span class="p">:</span>                  <span class="mf">0.512</span>
<span class="n">Method</span><span class="p">:</span>                 <span class="n">Least</span> <span class="n">Squares</span>   <span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">:</span>                 <span class="mf">1.895e+04</span>
<span class="n">Date</span><span class="p">:</span>                <span class="n">Tue</span><span class="p">,</span> <span class="mi">02</span> <span class="n">Aug</span> <span class="mi">2016</span>   <span class="n">Prob</span> <span class="p">(</span><span class="n">F</span><span class="o">-</span><span class="n">statistic</span><span class="p">):</span>               <span class="mf">0.00</span>
<span class="n">Time</span><span class="p">:</span>                        <span class="mi">17</span><span class="p">:</span><span class="mi">10</span><span class="p">:</span><span class="mi">34</span>   <span class="n">Log</span><span class="o">-</span><span class="n">Likelihood</span><span class="p">:</span>                <span class="o">-</span><span class="mf">51812.</span>
<span class="n">No</span><span class="o">.</span> <span class="n">Observations</span><span class="p">:</span>               <span class="mi">18067</span>   <span class="n">AIC</span><span class="p">:</span>                         <span class="mf">1.036e+05</span>
<span class="n">Df</span> <span class="n">Residuals</span><span class="p">:</span>                   <span class="mi">18065</span>   <span class="n">BIC</span><span class="p">:</span>                         <span class="mf">1.036e+05</span>
<span class="n">Df</span> <span class="n">Model</span><span class="p">:</span>                           <span class="mi">1</span>
<span class="n">Covariance</span> <span class="n">Type</span><span class="p">:</span>            <span class="n">nonrobust</span>
<span class="o">==============================================================================</span>
<span class="n">coef</span>    <span class="n">std</span> <span class="n">err</span>          <span class="n">t</span>      <span class="n">P</span><span class="o">&gt;|</span><span class="n">t</span><span class="o">|</span>      <span class="p">[</span><span class="mf">95.0</span><span class="o">%</span> <span class="n">Conf</span><span class="o">.</span> <span class="n">Int</span><span class="o">.</span><span class="p">]</span>
<span class="o">------------------------------------------------------------------------------</span>
<span class="n">Intercept</span>      <span class="mf">2.2523</span>      <span class="mf">0.054</span>     <span class="mf">41.656</span>      <span class="mf">0.000</span>         <span class="mf">2.146</span>     <span class="mf">2.358</span>
<span class="n">depth</span>         <span class="mf">11.5836</span>      <span class="mf">0.084</span>    <span class="mf">137.675</span>      <span class="mf">0.000</span>        <span class="mf">11.419</span>    <span class="mf">11.749</span>
<span class="o">==============================================================================</span>
<span class="n">Omnibus</span><span class="p">:</span>                    <span class="mf">12117.030</span>   <span class="n">Durbin</span><span class="o">-</span><span class="n">Watson</span><span class="p">:</span>                   <span class="mf">0.673</span>
<span class="n">Prob</span><span class="p">(</span><span class="n">Omnibus</span><span class="p">):</span>                  <span class="mf">0.000</span>   <span class="n">Jarque</span><span class="o">-</span><span class="n">Bera</span> <span class="p">(</span><span class="n">JB</span><span class="p">):</span>           <span class="mf">391356.565</span>
<span class="n">Skew</span><span class="p">:</span>                           <span class="mf">2.771</span>   <span class="n">Prob</span><span class="p">(</span><span class="n">JB</span><span class="p">):</span>                         <span class="mf">0.00</span>
<span class="n">Kurtosis</span><span class="p">:</span>                      <span class="mf">25.117</span>   <span class="n">Cond</span><span class="o">.</span> <span class="n">No</span><span class="o">.</span>                         <span class="mf">3.46</span>
<span class="o">==============================================================================</span>

<span class="n">Warnings</span><span class="p">:</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="n">Standard</span> <span class="n">Errors</span> <span class="n">assume</span> <span class="n">that</span> <span class="n">the</span> <span class="n">covariance</span> <span class="n">matrix</span> <span class="n">of</span> <span class="n">the</span> <span class="n">errors</span> <span class="ow">is</span> <span class="n">correctly</span> <span class="n">specified</span><span class="o">.</span>
</pre></div>
</div>
<p>or sci-kit learn package</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>

<span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span> <span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="n">LinearRegression</span><span class="p">(</span><span class="n">copy_X</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>
<span class="n">array</span><span class="p">([</span> <span class="mf">0.5</span><span class="p">,</span>  <span class="mf">0.5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="ridge-regression">
<h3>7.2.2. Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">¶</a></h3>
<p><strong>Regularisaton</strong> is an important concept used in Ridge Regression as well as the next LASSO regression.
Ridge regression uses regularisation which adds a penalty parameter to a variable when it has a large variation.
Regularisation prevents overfitting by restricting the model, thus lowering its complexity.</p>
<blockquote>
<div><ul class="simple">
<li>Uses L2 regularisation, which <em>reduces the sum of squares</em> of the parameters</li>
<li>The influence of L2 is controlled by an alpha parameter. Default is 1.</li>
<li>High alpha means more regularisation and a simpler model.</li>
<li>More in <a class="reference external" href="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/">https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/</a></li>
</ul>
</div></blockquote>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">#### IMPORT MODULES ####</span>
<span class="kn">import</span> <span class="nn">panda</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">MinMaxScaler</span>

<span class="c1">#### TRAIN-TEST SPLIT ####</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_crime</span><span class="p">,</span> <span class="n">y_crime</span><span class="p">,</span>
                                                 <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1">#### NORMALIZATION ####</span>
  <span class="c1"># using minmaxscaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>


<span class="c1">#### CREATE AND FIT MODEL ####</span>
<span class="n">linridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">20.0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Crime dataset&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ridge regression linear model intercept: </span><span class="si">{}</span><span class="s1">&#39;</span>
   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ridge regression linear model coeff:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span>
   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R-squared score (training): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span>
   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R-squared score (test): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span>
   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of non-zero features: </span><span class="si">{}</span><span class="s1">&#39;</span>
   <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
<p>To investigate the effect of alpha:</p>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Ridge regression: effect of alpha regularization parameter</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">this_alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]:</span>
  <span class="n">linridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">this_alpha</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">r2_train</span> <span class="o">=</span> <span class="n">linridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
  <span class="n">r2_test</span> <span class="o">=</span> <span class="n">linridge</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
  <span class="n">num_coeff_bigger</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">linridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Alpha = </span><span class="si">{:.2f}</span><span class="se">\n</span><span class="s1">num abs(coeff) &gt; 1.0: </span><span class="si">{}</span><span class="s1">, </span><span class="se">\</span>
<span class="s1">        r-squared training: </span><span class="si">{:.2f}</span><span class="s1">, r-squared test: </span><span class="si">{:.2f}</span><span class="se">\n</span><span class="s1">&#39;</span>
       <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">this_alpha</span><span class="p">,</span> <span class="n">num_coeff_bigger</span><span class="p">,</span> <span class="n">r2_train</span><span class="p">,</span> <span class="n">r2_test</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last simple">
<li>Many variables with small/medium effects: Ridge</li>
<li>Only a few variables with medium/large effects: LASSO</li>
</ul>
</div>
</div>
<div class="section" id="lasso-regression">
<h3>7.2.3. LASSO Regression<a class="headerlink" href="#lasso-regression" title="Permalink to this headline">¶</a></h3>
<p>LASSO refers to Least Absolute Shrinkage and Selection Operator Regression.
Like Ridge Regression this also has a regularisation property.</p>
<ul class="simple">
<li>Uses L1 regularisation, which <em>reduces sum of the absolute values of coefficients</em>, that change unimportant features (their regression coefficients) into 0</li>
<li>This is known as a sparse solution, or a kind of feature selection, since some variables were removed in the process</li>
<li>The influence of L1 is controlled by an alpha parameter. Default is 1.</li>
<li>High alpha means more regularisation and a simpler model. When alpha = 0, then it is a normal OLS regression.</li>
</ul>
<ol class="loweralpha simple">
<li>Bias increase &amp; variability decreases when alpha increases.</li>
<li>Useful when there are many features (explanatory variables).</li>
<li>Have to standardize all features so that they have mean 0 and std error 1.</li>
<li>Have several algorithms: LAR (Least Angle Regression). Starts w 0 predictors &amp; add each predictor that is most correlated at each step.</li>
</ol>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="c1">#### IMPORT MODULES ####</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">py</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LassoLarsCV</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_boston</span>



<span class="c1">#### NORMALIZATION ####</span>
<span class="c1"># standardise the means to 0 and standard error to 1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span> <span class="c1"># df.columns[:-1] = dataframe for all features</span>
  <span class="n">df</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float64&#39;</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>



<span class="c1">#### TRAIN TEST SPLIT ####</span>
<span class="n">train_feature</span><span class="p">,</span> <span class="n">test_feature</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> \
<span class="n">train_test_split</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">train_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">test_feature</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="mi">404</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="p">(</span><span class="mi">102</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>



<span class="c1">#### CREATE MODEL ####</span>
<span class="c1"># Fit the LASSO LAR regression model</span>
<span class="c1"># cv=10; use k-fold cross validation</span>
<span class="c1"># precompute; True=model will be faster if dataset is large</span>
<span class="n">model</span><span class="o">=</span><span class="n">LassoLarsCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">precompute</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>



<span class="c1">#### FIT MODEL ####</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_feature</span><span class="p">,</span><span class="n">train_target</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="n">model</span>
<span class="n">LassoLarsCV</span><span class="p">(</span><span class="n">copy_X</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">2.2204460492503131e-16</span><span class="p">,</span>
      <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">max_n_alphas</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
      <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">positive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">precompute</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>



<span class="c1">#### ANALYSE COEFFICIENTS ####</span>
<span class="n">Compare</span> <span class="n">the</span> <span class="n">regression</span> <span class="n">coefficients</span><span class="p">,</span> <span class="ow">and</span> <span class="n">see</span> <span class="n">which</span> <span class="n">one</span> <span class="n">LASSO</span> <span class="n">removed</span><span class="o">.</span>
<span class="n">LSTAT</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">most</span> <span class="n">important</span> <span class="n">predictor</span><span class="p">,</span> <span class="n">followed</span> <span class="n">by</span> <span class="n">RM</span><span class="p">,</span> <span class="n">DIS</span><span class="p">,</span> <span class="ow">and</span> <span class="n">RAD</span><span class="o">.</span> <span class="n">AGE</span> <span class="ow">is</span> <span class="n">removed</span> <span class="n">by</span> <span class="n">LASSO</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">df2</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">feature</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">df2</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">RM</span>    <span class="mf">3.050843</span>
<span class="n">RAD</span>   <span class="mf">2.040252</span>
<span class="n">ZN</span>    <span class="mf">1.004318</span>
<span class="n">B</span>     <span class="mf">0.629933</span>
<span class="n">CHAS</span>  <span class="mf">0.317948</span>
<span class="n">INDUS</span> <span class="mf">0.225688</span>
<span class="n">AGE</span>   <span class="mf">0.000000</span>
<span class="n">CRIM</span>  <span class="o">-</span><span class="mf">0.770291</span>
<span class="n">NOX</span>   <span class="o">-</span><span class="mf">1.617137</span>
<span class="n">TAX</span>   <span class="o">-</span><span class="mf">1.731576</span>
<span class="n">PTRATIO</span>       <span class="o">-</span><span class="mf">1.923485</span>
<span class="n">DIS</span>   <span class="o">-</span><span class="mf">2.733660</span>
<span class="n">LSTAT</span> <span class="o">-</span><span class="mf">3.878356</span>



<span class="c1">#### SCORE MODEL ####</span>
<span class="c1"># MSE from training and test data</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">mean_squared_error</span>
<span class="n">train_error</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">train_target</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">train_feature</span><span class="p">))</span>
<span class="n">test_error</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">test_target</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_feature</span><span class="p">))</span>

<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;training data MSE&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_error</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;test data MSE&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test_error</span><span class="p">)</span>

<span class="c1"># MSE closer to 0 are better</span>
<span class="c1"># test dataset is less accurate as expected</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">training</span> <span class="n">data</span> <span class="n">MSE</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">20.7279948891</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">test</span> <span class="n">data</span> <span class="n">MSE</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">28.3767672242</span>


<span class="c1"># R-square from training and test data</span>
<span class="n">rsquared_train</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_feature</span><span class="p">,</span><span class="n">train_target</span><span class="p">)</span>
<span class="n">rsquared_test</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_feature</span><span class="p">,</span><span class="n">test_target</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;training data R-square&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rsquared_train</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;test data R-square&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rsquared_test</span><span class="p">)</span>

<span class="c1"># test data explained 65% of the predictors</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">training</span> <span class="n">data</span> <span class="n">R</span><span class="o">-</span><span class="n">square</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">0.755337444405</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">test</span> <span class="n">data</span> <span class="n">R</span><span class="o">-</span><span class="n">square</span>
<span class="o">&gt;&gt;&gt;</span> <span class="mf">0.657019301268</span>
</pre></div>
</div>
</div>
<div class="section" id="polynomial-regression">
<h3>7.2.4. Polynomial Regression<a class="headerlink" href="#polynomial-regression" title="Permalink to this headline">¶</a></h3>
<div class="code python highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">PolynomialFeatures</span>


<span class="c1"># Normal Linear Regression</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_F1</span><span class="p">,</span> <span class="n">y_F1</span><span class="p">,</span>
                                                   <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">linreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;linear model coeff (w): </span><span class="si">{}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;linear model intercept (b): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R-squared score (training): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R-squared score (test): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Now we transform the original input data to add</span><span class="se">\n\</span>
<span class="s1">polynomial features up to degree 2 (quadratic)</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Polynomial Regression</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_F1_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_F1</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_F1_poly</span><span class="p">,</span> <span class="n">y_F1</span><span class="p">,</span>
                                                   <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">linreg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(poly deg 2) linear model coeff (w):</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(poly deg 2) linear model intercept (b): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(poly deg 2) R-squared score (training): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(poly deg 2) R-squared score (test): </span><span class="si">{:.3f}</span><span class="se">\n</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>

<span class="c1"># Polynomial with Ridge Regression</span>
<span class="sd">&#39;&#39;&#39;Addition of many polynomial features often leads to</span>
<span class="sd">overfitting, so we often use polynomial features in combination</span>
<span class="sd">with regression that has a regularization penalty, like ridge</span>
<span class="sd">regression.&#39;&#39;&#39;</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_F1_poly</span><span class="p">,</span> <span class="n">y_F1</span><span class="p">,</span>
                                                   <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">linreg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(poly deg 2 + ridge) linear model coeff (w):</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(poly deg 2 + ridge) linear model intercept (b): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(poly deg 2 + ridge) R-squared score (training): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;(poly deg 2 + ridge) R-squared score (test): </span><span class="si">{:.3f}</span><span class="s1">&#39;</span>
     <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">linreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="unsupervised.html" class="btn btn-neutral float-right" title="8. Unsupervised Learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="association.html" class="btn btn-neutral" title="6. Tests of Association" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Jake Teo.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>